{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 주요 라이브러리\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from pydub import AudioSegment\n",
    "from peft import PromptEncoder, PromptEncoderConfig\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "class JSONAudioDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)[\"data\"]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        audio_path = sample[\"audio_file\"]\n",
    "        text_condition = f\"{sample['description']} Keywords: {', '.join(sample['keywords'])}. Moods: {', '.join(sample['moods'])}.\"\n",
    "        audio = self.load_audio(audio_path)\n",
    "        return audio, text_condition\n",
    "\n",
    "    def load_audio(self, path):\n",
    "        # MP3를 WAV로 변환\n",
    "        if path.endswith(\".mp3\"):\n",
    "            wav_path = path.replace(\".mp3\", \".wav\")\n",
    "            if not os.path.exists(wav_path):\n",
    "                self.convert_mp3_to_wav(path, wav_path)\n",
    "            path = wav_path\n",
    "        try:\n",
    "            audio, _ = librosa.load(path, sr=32000)\n",
    "            return torch.tensor(audio)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {path}: {e}\")\n",
    "            return torch.zeros(1)  # 빈 텐서 반환\n",
    "\n",
    "    def convert_mp3_to_wav(self, mp3_path, wav_path):\n",
    "        try:\n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            audio.export(wav_path, format=\"wav\")\n",
    "            print(f\"Converted {mp3_path} to {wav_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {mp3_path} to WAV: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 프롬프트 조건 공급자 정의\n",
    "class PEFTPConditionProvider(torch.nn.Module):\n",
    "    def __init__(self, prompt_length, hidden_size, num_transformer_submodules, num_attention_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.config = PromptEncoderConfig(\n",
    "            task_type=\"TEXT_GENERATION\",\n",
    "            num_virtual_tokens=prompt_length,\n",
    "            token_dim=hidden_size,\n",
    "            encoder_hidden_size=hidden_size,\n",
    "            encoder_num_layers=2,\n",
    "            encoder_dropout=0.1,\n",
    "            num_transformer_submodules=num_transformer_submodules\n",
    "        )\n",
    "        self.prompt_encoder = PromptEncoder(self.config)\n",
    "        self.num_virtual_tokens = prompt_length\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        batch_size = tokens.size(0)\n",
    "        indices = torch.arange(self.num_virtual_tokens, device=tokens.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        prompt_embeds = self.prompt_encoder(indices)\n",
    "        if len(prompt_embeds.shape) == 4:\n",
    "            prompt_embeds = prompt_embeds.squeeze(0)\n",
    "        return torch.cat([prompt_embeds, tokens], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 정의\n",
    "def train_model(model, tokenizer, dataloader, device, epochs, grad_acc_steps, lr, checkpoint_dir):\n",
    "    optimizer = AdamW(\n",
    "        list(model.lm.parameters()) + list(model.condition_provider.parameters()), \n",
    "        lr=lr\n",
    "    )\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    model.lm.train()\n",
    "    model.condition_provider.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (audio, text) in enumerate(dataloader):\n",
    "            audio = audio.to(device)\n",
    "            text_tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "            prompts = model.condition_provider(text_tokens)\n",
    "\n",
    "            num_codebooks = model.lm.num_codebooks\n",
    "            hidden_size = model.lm.embedding_dim if hasattr(model.lm, \"embedding_dim\") else 768\n",
    "            audio = audio.unsqueeze(1).expand(-1, num_codebooks, -1).to(torch.long)\n",
    "\n",
    "            try:\n",
    "                outputs = model.lm(audio, prompts)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                raise e\n",
    "\n",
    "            loss = loss_fn(outputs, audio)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            if (i + 1) % grad_acc_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.lm.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음악 생성 함수 정의\n",
    "def generate_music(model, tokenizer, text_condition, device, output_path):\n",
    "    model.eval()\n",
    "    tokenized = tokenizer(text_condition, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokens = tokenized.input_ids.to(device)\n",
    "    prompts = model.condition_provider(tokens)\n",
    "    with torch.no_grad():\n",
    "        generated_audio = model.generate(prompts)\n",
    "    sf.write(output_path, generated_audio.cpu().numpy(), samplerate=32000)\n",
    "    print(f\"Generated music saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mcondition_provider \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcondition_provider\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 29\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mJSONAudioDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJSON_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m train_model(model, tokenizer, dataloader, DEVICE, EPOCHS, GRAD_ACC_STEPS, LR, CHECKPOINT_DIR)\n",
      "Cell \u001b[1;32mIn[37], line 10\u001b[0m, in \u001b[0;36mJSONAudioDataset.__init__\u001b[1;34m(self, json_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, json_path):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data'"
     ]
    }
   ],
   "source": [
    "# 주요 실행 코드\n",
    "JSON_PATH = \"data\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACC_STEPS = 1\n",
    "LR = 1e-4\n",
    "PROMPT_LENGTH = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from audiocraft.models import MusicGen\n",
    "\n",
    "model = MusicGen.get_pretrained(\"facebook/musicgen-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "prompt_provider = PEFTPConditionProvider(\n",
    "    prompt_length=PROMPT_LENGTH,\n",
    "    hidden_size=768,\n",
    "    num_transformer_submodules=12,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    ")\n",
    "model.condition_provider = prompt_provider\n",
    "model.lm = model.lm.to(DEVICE)\n",
    "model.condition_provider = model.condition_provider.to(DEVICE)\n",
    "\n",
    "dataset = JSONAudioDataset(JSON_PATH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_model(model, tokenizer, dataloader, DEVICE, EPOCHS, GRAD_ACC_STEPS, LR, CHECKPOINT_DIR)\n",
    "\n",
    "generate_music(model, tokenizer, \"A warm and cozy winter melody.\", DEVICE, \"generated_music.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
