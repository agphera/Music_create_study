{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import 주요 라이브러리\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from pydub import AudioSegment\n",
    "from peft import PromptEncoder, PromptEncoderConfig\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class JSONAudioDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)[\"data\"]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        audio_path = sample[\"audio_file\"]\n",
    "        text_condition = f\"{sample['description']} Keywords: {', '.join(sample['keywords'])}. Moods: {', '.join(sample['moods'])}.\"\n",
    "        audio = self.load_audio(audio_path)\n",
    "        return audio, text_condition\n",
    "\n",
    "    def load_audio(self, path):\n",
    "        if path.endswith(\".mp3\"):\n",
    "            wav_path = path.replace(\".mp3\", \".wav\")\n",
    "            if not os.path.exists(wav_path):\n",
    "                self.convert_mp3_to_wav(path, wav_path)\n",
    "            path = wav_path\n",
    "        try:\n",
    "            audio, _ = librosa.load(path, sr=32000)\n",
    "            return torch.tensor(audio)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {path}: {e}\")\n",
    "            return torch.zeros(1)\n",
    "\n",
    "    def convert_mp3_to_wav(self, mp3_path, wav_path):\n",
    "        try:\n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            audio.export(wav_path, format=\"wav\")\n",
    "            print(f\"Converted {mp3_path} to {wav_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {mp3_path} to WAV: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 프롬프트 조건 공급자 정의\n",
    "class PEFTPConditionProvider(torch.nn.Module):\n",
    "    def __init__(self, prompt_length, hidden_size, num_transformer_submodules, num_attention_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.config = PromptEncoderConfig(\n",
    "            task_type=\"TEXT_GENERATION\",\n",
    "            num_virtual_tokens=prompt_length,\n",
    "            token_dim=hidden_size,\n",
    "            encoder_hidden_size=hidden_size,\n",
    "            encoder_num_layers=2,\n",
    "            encoder_dropout=0.1,\n",
    "            num_transformer_submodules=num_transformer_submodules\n",
    "        )\n",
    "        self.prompt_encoder = PromptEncoder(self.config)\n",
    "        self.num_virtual_tokens = prompt_length\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        batch_size = tokens.size(0)\n",
    "        indices = torch.arange(self.num_virtual_tokens, device=tokens.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        prompt_embeds = self.prompt_encoder(indices)\n",
    "        if len(prompt_embeds.shape) == 4:\n",
    "            prompt_embeds = prompt_embeds.squeeze(0)\n",
    "        return torch.cat([prompt_embeds, tokens], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 정의\n",
    "def train_model(model, tokenizer, dataloader, device, epochs, grad_acc_steps, lr, checkpoint_dir):\n",
    "    optimizer = AdamW(\n",
    "        list(model.lm.parameters()) + list(model.condition_provider.parameters()), \n",
    "        lr=lr\n",
    "    )\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    model.lm.train()\n",
    "    model.condition_provider.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (audio, text) in enumerate(dataloader):\n",
    "            audio = audio.to(device)\n",
    "            text_tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "            prompts = model.condition_provider(text_tokens)\n",
    "\n",
    "            num_codebooks = model.lm.num_codebooks\n",
    "            hidden_size = model.lm.embedding_dim if hasattr(model.lm, \"embedding_dim\") else 768\n",
    "            audio = audio.unsqueeze(1).expand(-1, num_codebooks, -1).to(torch.long)\n",
    "\n",
    "            try:\n",
    "                outputs = model.lm(audio, prompts)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                raise e\n",
    "\n",
    "            loss = loss_fn(outputs, audio)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            if (i + 1) % grad_acc_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.lm.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음악 생성 함수 정의\n",
    "def generate_music(model, tokenizer, text_condition, device, output_path):\n",
    "    model.eval()\n",
    "    tokenized = tokenizer(text_condition, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokens = tokenized.input_ids.to(device)\n",
    "    prompts = model.condition_provider(tokens)\n",
    "    with torch.no_grad():\n",
    "        generated_audio = model.generate(prompts)\n",
    "    sf.write(output_path, generated_audio.cpu().numpy(), samplerate=32000)\n",
    "    print(f\"Generated music saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\audiocraft\\models\\musicgen.py:83: UserWarning: MusicGen pretrained model relying on deprecated checkpoint mapping. Please use full pre-trained id instead: facebook/musicgen-small\n",
      "  warnings.warn(\n",
      "c:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting Silent-Night.mp3 to WAV: [Errno 2] No such file or directory: 'Silent-Night.mp3'\n",
      "Error loading audio file Silent-Night.wav: [Errno 2] No such file or directory: 'Silent-Night.wav'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dana\\AppData\\Local\\Temp\\ipykernel_48744\\3749965209.py:25: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(path, sr=32000)\n",
      "c:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m dataset \u001b[38;5;241m=\u001b[39m JSONAudioDataset(JSON_PATH)\n\u001b[0;32m     30\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGRAD_ACC_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m generate_music(model, tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA warm and cozy winter melody.\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEVICE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_music.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, tokenizer, dataloader, device, epochs, grad_acc_steps, lr, checkpoint_dir)\u001b[0m\n\u001b[0;32m     16\u001b[0m audio \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 18\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondition_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m num_codebooks \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mnum_codebooks\n\u001b[0;32m     21\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39membedding_dim \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mlm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m768\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dana\\anaconda3\\envs\\musicgen2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mPEFTPConditionProvider.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_embeds\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m     22\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m prompt_embeds\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# 주요 실행 코드\n",
    "JSON_PATH = \"data/Silent-Night.json\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACC_STEPS = 1\n",
    "LR = 1e-4\n",
    "PROMPT_LENGTH = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from audiocraft.models import MusicGen\n",
    "\n",
    "model = MusicGen.get_pretrained(\"facebook/musicgen-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "prompt_provider = PEFTPConditionProvider(\n",
    "    prompt_length=PROMPT_LENGTH,\n",
    "    hidden_size=768,\n",
    "    num_transformer_submodules=12,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    ")\n",
    "model.condition_provider = prompt_provider\n",
    "model.lm = model.lm.to(DEVICE)\n",
    "model.condition_provider = model.condition_provider.to(DEVICE)\n",
    "\n",
    "dataset = JSONAudioDataset(JSON_PATH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_model(model, tokenizer, dataloader, DEVICE, EPOCHS, GRAD_ACC_STEPS, LR, CHECKPOINT_DIR)\n",
    "\n",
    "generate_music(model, tokenizer, \"A warm and cozy winter melody.\", DEVICE, \"generated_music.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
